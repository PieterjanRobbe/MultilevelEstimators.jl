<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example · MultilevelEstimators.jl</title><meta name="title" content="Example · MultilevelEstimators.jl"/><meta property="og:title" content="Example · MultilevelEstimators.jl"/><meta property="twitter:title" content="Example · MultilevelEstimators.jl"/><meta name="description" content="Documentation for MultilevelEstimators.jl."/><meta property="og:description" content="Documentation for MultilevelEstimators.jl."/><meta property="twitter:description" content="Documentation for MultilevelEstimators.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="index.html"><img src="assets/logo.png" alt="MultilevelEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="index.html">MultilevelEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="index.html">Home</a></li><li class="is-active"><a class="tocitem" href="example.html">Example</a><ul class="internal"><li><a class="tocitem" href="#Lognormal-diffusion-problems"><span>Lognormal diffusion problems</span></a></li><li><a class="tocitem" href="#Multilevel-Monte-Carlo"><span>Multilevel Monte Carlo</span></a></li><li><a class="tocitem" href="#Multilevel-Quasi-Monte-Carlo"><span>Multilevel Quasi-Monte Carlo</span></a></li><li><a class="tocitem" href="#Multi-Index-Monte-Carlo"><span>Multi-Index Monte Carlo</span></a></li><li><a class="tocitem" href="#Unbiased-estimation"><span>Unbiased estimation</span></a></li></ul></li><li><a class="tocitem" href="manual.html">Manual</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="example.html">Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="example.html">Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/PieterjanRobbe/MultilevelEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/PieterjanRobbe/MultilevelEstimators.jl/blob/master/docs/src/example.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h1><p>We consider the elliptic PDE with random coefficients</p><p class="math-container">\[- \nabla ( a(x, \omega) \nabla u(x, \omega) ) = f(x)\]</p><p>defined on the unit square <span>$D = [0, 1]^2$</span>, with homogeneous boundary conditions <span>$u(x, \cdot) = 0$</span> on <span>$\partial D$</span>. </p><p>Assume the uncertain diffusion coefficient is given as a lognormal random field  <span>$\log a(x, \omega) = z(x, \omega)$</span> where <span>$z(x, \omega)$</span> is a Gaussian random field (see below).</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The following examples assume you have already installed the test dependencies as outlined in the <a href="index.html#Installation">Installation</a> guide.</p></div></div><ul><li><a href="example.html#Example">Example</a></li><li class="no-marker"><ul><li><a href="example.html#Lognormal-diffusion-problems">Lognormal diffusion problems</a></li><li><a href="example.html#Multilevel-Monte-Carlo">Multilevel Monte Carlo</a></li><li><a href="example.html#Multilevel-Quasi-Monte-Carlo">Multilevel Quasi-Monte Carlo</a></li><li><a href="example.html#Multi-Index-Monte-Carlo">Multi-Index Monte Carlo</a></li><li><a href="example.html#Unbiased-estimation">Unbiased estimation</a></li></ul></li></ul><h2 id="Lognormal-diffusion-problems"><a class="docs-heading-anchor" href="#Lognormal-diffusion-problems">Lognormal diffusion problems</a><a id="Lognormal-diffusion-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Lognormal-diffusion-problems" title="Permalink"></a></h2><p>First, we need to define a random field that will be used for the uncertain diffusion coefficient of the PDE. Fortunately, a Julia package that generates and samples from such random fields already exists.</p><pre><code class="language-julia hljs">using GaussianRandomFields</code></pre><p>To specify the spatial correlation in the random field, we consider the <a href="https://en.wikipedia.org/wiki/Matérn_covariance_function">Matérn</a> covariance function with a given correlation length and smoothness. This function is predefined in the package. </p><pre><code class="language-julia hljs">correlation_length = 0.5
smoothness = 2.0
covariance_function = CovarianceFunction(2, Matern(correlation_length, smoothness))</code></pre><p>We generate samples of this smooth random field using a truncated <a href="https://en.wikipedia.org/wiki/Karhunen–Loève_theorem">Karhunen-Loève expansion</a> with 250 terms. To this end, we discretize the domain <code>D</code> into a regular grid with grid size <span>$\Delta x = \Delta y =$</span> 1/128.</p><pre><code class="language-julia hljs">n = 256
pts = range(0, stop=1, length=n)
n_kl = 250
grf = GaussianRandomField(covariance_function, KarhunenLoeve(n_kl), pts, pts)</code></pre><p>A sample of this Gaussian random field is returned by either calling the <code>sample</code>-function directly, or by supplying the random numbers used for sampling from the random field.</p><pre><code class="language-julia hljs">sample(grf)
sample(grf, xi = randn(n_kl))</code></pre><p>The keyword <code>xi</code> corresponds to the sample <span>$\omega$</span> in the defintion of the PDE.</p><p>We vizualize a sample of this random field.</p><pre><code class="language-julia hljs">using Plots
contourf(sample(grf))</code></pre><img src="assets/grf.png" width="400"><p>Many more options and details regarding the generation of Gaussian random fields can be found in the <a href="https://github.com/PieterjanRobbe/GaussianRandomFields.jl/blob/master/tutorial/tutorial.ipynb">tutorial</a> that accomagnies the package <a href="https://github.com/PieterjanRobbe/GaussianRandomFields.jl"><code>GaussianRandomFields.jl</code></a>. </p><p>Suppose we are interested in computing the expected value of a quantity of interest derived from the solution of the PDE. For example, we might be interested in the value of the solution <span>$u(x, ω)$</span> at the point <span>$x=y=$</span>0.5.</p><h2 id="Multilevel-Monte-Carlo"><a class="docs-heading-anchor" href="#Multilevel-Monte-Carlo">Multilevel Monte Carlo</a><a id="Multilevel-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Multilevel-Monte-Carlo" title="Permalink"></a></h2><p>The basics of any <a href="https://en.wikipedia.org/wiki/Multilevel_Monte_Carlo_method">multilevel method</a> is a hierarchy of approximations of the model problem with an increasing accuracy, but corresponding increasing cost. For the lognormal diffusion example, such a hierarchy is provided by solving the PDE on an ever finer grid. We define a hierarchy of Gaussian random field generators using</p><pre><code class="language-julia hljs">grfs = Vector{typeof(grf)}(undef, 7)
for i in 1:7
    n = 2^(i+1)
    pts = 1/n:1/n:1-1/n
    grfs[i] = GaussianRandomField(covariance_function, KarhunenLoeve(n_kl), pts, pts)
end</code></pre><p>The corresponding system matrix that results from a finite difference discretization of the PDE with varying diffusion coefficient is returned by the <code>elliptic2d</code>-function from the <a href="https://github.com/PieterjanRobbe/SimpleMultigrid.jl"><code>SimpleMultigrid</code></a> package.</p><pre><code class="language-julia hljs">using SimpleMultigrid
z = sample(grf)
a = exp.(z)
A = elliptic2d(a)
b = ones(size(A, 1))
u = A\b
contourf(reshape(u, n, n))</code></pre><h3 id="Creating-an-Estimator"><a class="docs-heading-anchor" href="#Creating-an-Estimator">Creating an <code>Estimator</code></a><a id="Creating-an-Estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-an-Estimator" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MultilevelEstimators</code></pre><p>The main type provided by MultilevelEstimators is an <a href="manual.html#Estimator"><code>Estimator</code></a>. This type has two parametric subtypes, <code>Estimator{&lt;:AbstractIndexSet, &lt;:AbstractSampleMethod}</code>, where <code>AbstractIndexSet</code> is an abstract type for the  index set, and <code>AbstractSampleMethod</code> is an abstract type for the sample method. For example, a Multilevel Monte Carlo method will have type signature <code>Estimator{ML, MC}</code>.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>For a complete list of possible index set types, see <a href="manual.html#IndexSet"><code>IndexSet</code></a>. For a complete list of possible sample method types, see <a href="manual.html#SampleMethod"><code>SampleMethod</code></a>.</p></div></div><p>The main user interaction required by MultilevelEstimators is the definition of a sample function. This function must return a sample of the quantity of interest, and of its difference, for the given discretization parameter (or level) and random parameters. Here is an example for the lognormal diffusion problem:</p><pre><code class="language-julia hljs">function sample_lognormal(level::Level, ω::Vector{&lt;:Real}, grf::GaussianRandomField)

    # solve on finest grid
    z  = sample(grf, xi = ω)
    af = exp.(z)
    Af = elliptic2d(af)
    bf = fill(one(eltype(Af)), size(Af, 1))
    uf = Af\bf
    Qf = uf[length(uf) ÷ 2]

    # compute difference when not on coarsest grid
    dQ = Qf
    if level != Level(0)
        ac = view(af, 2:2:size(af, 1), 2:2:size(af, 2))
        Ac = elliptic2d(ac)
        bc = fill(one(eltype(Af)), size(Ac, 1))
        uc = Ac\bc
        Qc = uc[length(uc) ÷ 2]
        dQ -= Qc
    end
    dQ, Qf
end</code></pre><p>Thus, when the level parameter is zero, we return the value of the quantity of interest on the coarsest mesh. For a larger level parameter, we return both the difference of the quantity of interest with a coarser grid solution, and the value of the quantity of interest itself.</p><p>The sample function can only have two input parameters: a level and a vector of random parameters. For sample functions that require data (such as the precomputed Gaussian random fields in the example above), one might need to add a convenience function:</p><pre><code class="language-julia hljs">sample_lognormal(level, ω) = sample_lognormal(level, ω, grfs[level + one(level)])</code></pre><p>Finally, for the construction of an <code>Estimator</code>, we also need to specify the number of random parameters and their respective distributions. For the lognormal diffusion problem, these random parameters are normally distributed.</p><pre><code class="language-julia hljs">distributions = [Normal() for i in 1:n_kl]</code></pre><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>A complete list of predefined distributions can be found in the manual for <a href="manual.html#Distribution"><code>Distribution</code>s</a>.</p></div></div><p>An estimator for the lognormal diffusion problem can thus be created using</p><pre><code class="language-julia hljs">estimator = Estimator(ML(), MC(), sample_lognormal, distributions)</code></pre><h3 id="Specifying-options"><a class="docs-heading-anchor" href="#Specifying-options">Specifying options</a><a id="Specifying-options-1"></a><a class="docs-heading-anchor-permalink" href="#Specifying-options" title="Permalink"></a></h3><p>Different options and settings can be passed on to the <code>Estimator</code> by supplying the appropriate keyword argument. For example, to set the number of warm up samples to 10, call</p><pre><code class="language-julia hljs">estimator = Estimator(ML(), MC(), sample_lognormal, distributions, nb_of_warm_up_samples = 10)</code></pre><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>A complete list of optional arguments can be found in the manual for <a href="manual.html#Estimator"><code>Estimator</code></a>.</p></div></div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>It is easy to extend the current setup to the computation of the expected value of multiple quantities of interest, by using the keyword <code>nb_of_qoi</code>.</p></div></div><h3 id="Running-a-simulation"><a class="docs-heading-anchor" href="#Running-a-simulation">Running a simulation</a><a id="Running-a-simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Running-a-simulation" title="Permalink"></a></h3><p>To start a simulation for the expected value of the quantity of interest up to an absolute (root mean square) error of <code>5e-3</code>, call</p><pre><code class="language-julia hljs">h = run(estimator, 5e-3)</code></pre><p>This function will return a <a href="manual.html#History"><code>History</code></a> object that contains usefull diagnostics about the simulation.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>See <a href="manual.html#History"><code>History</code></a> for a complete list of diagnostic entries.</p></div></div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>By default, the simulation is performed for a sequence of larger tolerances to get improved estimates for the rates of decay of expected value and variance, bias estimation... This sequence is of the form</p><p class="math-container">\[\text{tol}_i = p^{(n - 1 - i)} \text{tol}, i = 0, \ldots, n - 1\]</p><p>with <span>$p&gt;$</span>1 and where the values for <span>$p$</span> and <span>$n$</span> can be controlled by the keyword arguments <code>continuation_mul_factor</code> and <code>nb_of_tols</code> respectively. You can disable continuation with the optional argument <code>continuation = false</code>. Our experience is that continuation is a very powerful tool when combined with a non-trivial mean square error splitting (<code>do_mse_splitting = true</code>) and variance regression (<code>do_regression = true</code>). </p></div></div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>By default, samples will be taken in parallel on all available processors (see <code>addprocs</code>). The number of processors is controlled by the optional keyword <code>nb_of_workers</code>. This can either be a fixed value, or a function, specifying the number of workers to be used on each level.</p></div></div><h3 id="Vizualization-of-the-result-using-[Reporter](https://github.com/PieterjanRobbe/Reporter.jl)"><a class="docs-heading-anchor" href="#Vizualization-of-the-result-using-[Reporter](https://github.com/PieterjanRobbe/Reporter.jl)">Vizualization of the result using <a href="https://github.com/PieterjanRobbe/Reporter.jl"><code>Reporter</code></a></a><a id="Vizualization-of-the-result-using-[Reporter](https://github.com/PieterjanRobbe/Reporter.jl)-1"></a><a class="docs-heading-anchor-permalink" href="#Vizualization-of-the-result-using-[Reporter](https://github.com/PieterjanRobbe/Reporter.jl)" title="Permalink"></a></h3><p>MultilevelEstimators can automatically build a set of diagnostic figures based on a <a href="manual.html#History"><code>History</code></a> file. Load the package by</p><pre><code class="language-julia hljs">using Reporter</code></pre><p>and generate a report by calling</p><pre><code class="language-julia hljs">report(h)</code></pre><p>If all goes well, you should now see a webpage with diagnostic information about the simulation, similar to <a href="assets/UntitledEstimator/index.html">this one</a>. Print-ready <code>.tex</code>-figures are stored locally under <code>figures/</code>.</p><h2 id="Multilevel-Quasi-Monte-Carlo"><a class="docs-heading-anchor" href="#Multilevel-Quasi-Monte-Carlo">Multilevel Quasi-Monte Carlo</a><a id="Multilevel-Quasi-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Multilevel-Quasi-Monte-Carlo" title="Permalink"></a></h2><p><a href="https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method">Quasi-Monte Carlo</a> is an alternative way to pick the samples <code>omega</code>. The random samples in the Monte Carlo method are replaced by deterministically well-chosen points that increase the accuracy of the estimation with respect to the number of samples. A popular type of such point sets are <a href="https://people.cs.kuleuven.be/~dirk.nuyens/qmc-generators/">rank-1 lattice rules</a>, implemented here as <a href="manual.html#LatticeRule32"><code>LatticeRule32</code></a>. These points are used by default when calling the <code>QMC</code>-version of the <code>Estimator</code></p><pre><code class="language-julia hljs">estimator = Estimator(ML(), QMC(), sample_lognormal, distributions)</code></pre><p>Under the hood, the default lattice rule uses a 3600-dimensional generating vector from <a href="https://web.maths.unsw.edu.au/~fkuo/lattice/index.html">Kuo et al</a>. It is easy to provide your own generating vector with the optional <code>point_generator</code>-argument.</p><pre><code class="language-julia hljs">estimator = Estimator(ML(), QMC(), sample_lognormal, point_generator = LatticeRule32(&quot;my_gen_vec.txt&quot;))</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The number of shifts is controlled by the <code>nb_of_shifts</code>-keyword. We recommend a value between 10 (default) and 30. The number of samples is updated carefully, with a default multiplication factor of 1.2 (instead of the standard and theoretically justifiable value 2, which we found to be too aggressive). This number can be controlled with the keyword <code>sample_mul_factor</code>. </p></div></div><h2 id="Multi-Index-Monte-Carlo"><a class="docs-heading-anchor" href="#Multi-Index-Monte-Carlo">Multi-Index Monte Carlo</a><a id="Multi-Index-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Index-Monte-Carlo" title="Permalink"></a></h2><p>It is easy to extend the lognormal diffusion example to compute multi-index differences. Instead of a difference between a fine and a coarse approximation, we now compute mixed differences.</p><pre><code class="language-julia hljs">function sample_lognormal(index::Index, ω::Vector{&lt;:Real}, grf::GaussianRandomField)

    # solve on finest grid
    z  = sample(grf, xi = ω)
    af = exp.(z)
    Af = elliptic2d(af)
    bf = fill(one(eltype(Af)), size(Af, 1))
    uf = Af\bf
    Qf = uf[length(uf) ÷ 2]

    # compute multi-index differences
    dQ = Qf
    for (key, val) in diff(index)
        step = (index - key).I .+ 1
        ac = view(af, step[1]:step[1]:size(af, 1), step[2]:step[2]:size(af, 2))
        Ac = elliptic2d(ac)
        bc = fill(one(eltype(Af)), size(Ac, 1))
        uc = Ac\bc
        Qc = uc[length(uc) ÷ 2]
        dQ += val * Qc
    end
    dQ, Qf
end</code></pre><p>All functionality is hidden in the call to <code>diff(index)</code>. This function returns a <code>Dict</code> with as keys the indices where coarse approximations must be computed, and as values <code>+1</code> or <code>-1</code>, specifying how these corrections must be added to the quantity of interest on the fine grid.</p><h3 id="Multi-index-sets"><a class="docs-heading-anchor" href="#Multi-index-sets">Multi-index sets</a><a id="Multi-index-sets-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-index-sets" title="Permalink"></a></h3><p>Different choices for the multi-index sets are available: full tensor index sets (<a href="manual.html#MultilevelEstimators.FT"><code>FT</code></a>), total degree index sets (<a href="manual.html#MultilevelEstimators.TD"><code>TD</code></a>), hyperbolic cross index sets (<a href="manual.html#MultilevelEstimators.HC"><code>HC</code></a>) and Zaremba cross index sets (<a href="manual.html#MultilevelEstimators.ZC"><code>ZC</code></a>). For example, total degree (TD) index sets look like this </p><pre><code class="language-julia hljs">index_set = TD(2)
for sz = 0:5
    println(&quot;size parameter: &quot;, sz)
    print(index_set, sz)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">size parameter: 0
  ◼
size parameter: 1
  ◼
  ◼ ◼
size parameter: 2
  ◼
  ◼ ◼
  ◼ ◼ ◼
size parameter: 3
  ◼
  ◼ ◼
  ◼ ◼ ◼
  ◼ ◼ ◼ ◼
size parameter: 4
  ◼
  ◼ ◼
  ◼ ◼ ◼
  ◼ ◼ ◼ ◼
  ◼ ◼ ◼ ◼ ◼
size parameter: 5
  ◼
  ◼ ◼
  ◼ ◼ ◼
  ◼ ◼ ◼ ◼
  ◼ ◼ ◼ ◼ ◼
  ◼ ◼ ◼ ◼ ◼ ◼</code></pre><p>Weighted index sets are created by specifying appropriate weiths. All weights must be smaller than or equal to 1.</p><pre><code class="language-julia hljs">index_set = ZC(1/2, 1)
for sz = 0:5
    println(&quot;size parameter: &quot;, sz)
    print(index_set, sz)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">size parameter: 0
  ◼
size parameter: 1
  ◼
  ◼
size parameter: 2
  ◼
  ◼ ◼
  ◼ ◼
size parameter: 3
  ◼
  ◼
  ◼ ◼
  ◼ ◼
size parameter: 4
  ◼
  ◼
  ◼ ◼
  ◼ ◼ ◼
  ◼ ◼ ◼
size parameter: 5
  ◼
  ◼
  ◼
  ◼ ◼
  ◼ ◼ ◼
  ◼ ◼ ◼</code></pre><p>We can create an estimator the usual way:</p><pre><code class="language-julia hljs">estimator = Estimator(TD(2), MC(), sample_lognormal, distributions)</code></pre><p>where <code>TD(2)</code> denotes a 2-dimensional index set.</p><p>Before running a simulation, we must precomputed the random fields on all indices.</p><pre><code class="language-julia hljs">grfs = Matrix{typeof(grf)}(undef, 7, 7)
for index in get_index_set(TD(2), 6)
    n = 2 .^(index.I .+ 2)
    pts = map(i -&gt; range(1/i, step = 1/i, stop = 1 - 1/i), n)
    grfs[index + one(index)] = GaussianRandomField(covariance_function, KarhunenLoeve(n_kl), pts...)
end</code></pre><p>A multi-index Monte Carlo simulation with this estimator is performed by</p><pre><code class="language-julia hljs">h = run(estimator, 5e-3)</code></pre><h3 id="Adaptive-Multi-index-Monte-Carlo"><a class="docs-heading-anchor" href="#Adaptive-Multi-index-Monte-Carlo">Adaptive Multi-index Monte Carlo</a><a id="Adaptive-Multi-index-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Adaptive-Multi-index-Monte-Carlo" title="Permalink"></a></h3><p>Instead of using predefined (weighted or unweighted) index sets, we can also construct the index set adaptively based on some profit indicator (see [6])</p><p class="math-container">\[P_\ell = \frac{E_\ell}{\sqrt{V_\ell W_\ell}}\]</p><p>where <span>$E_\ell$</span> and <span>$V_\ell$</span> are the expected value and variance of the multi-index difference, respectively, and <span>$W_\ell$</span> is the computational cost required to computed a sample of the difference.</p><pre><code class="language-julia hljs">estimator = Estimator(AD(2), MC(), sample_lognormal, distributions)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The maximum allowed indices that can be simulated can be set by the keyword <code>max_search_space</code>. This keyword should be an index set of one of the above types. The adaptive method will only find indices inside this index set, with the size parameter equal to <code>max_index_set_param</code>.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>When the adaptive algorithm tries to add indices outside of the set of maximum allowed indices, a warning will be printed and this index is dropped when computing the bias. This might result in a biased estimator!</p></div></div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Sometimes it is beneficial to add a penalization parameter to the profit indicator to avoid searching too much around the coordinate axes, and bumping into a memory constraint:  </p><p class="math-container">\[P_\ell = \frac{E_\ell}{(\sqrt{V_\ell W_\ell})^p}\]</p><p>with 0 &lt; <span>$p$</span> &lt; 1. Specify <span>$p$</span> with the optional key <code>penalization</code>.</p></div></div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Another useful optional key is <code>acceptance_rate</code>. When this parameter is smaller than 1, a suboptimal index from the active set will be used for further refinement. This index is chosen according to an accept-reject method: we sample a uniform random number using <code>rand()</code>, and when this number is larger than the accept ratio, we pich another index at random. Lowering the <code>acceptance_rate</code> results in an algorithm with a stronger global search behavior, and is useful when the adaptive method is stuck along one or more directions. Default value is 1 (no globalization). </p></div></div><h3 id="Multi-Index-Quasi-Monte-Carlo"><a class="docs-heading-anchor" href="#Multi-Index-Quasi-Monte-Carlo">Multi-Index Quasi Monte Carlo</a><a id="Multi-Index-Quasi-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Index-Quasi-Monte-Carlo" title="Permalink"></a></h3><p>Just as in standard Multilevel Monte Carlo, the Multi-Index Monte Carlo method can be extended to use quasi-random numbers instead, see [5]. The setup remains the same, just change <code>MC()</code> into <code>QMC()</code>. For example, a Hyperbolic Cross Multi-Index Quasi-Monte Carlo estimator can be created by </p><pre><code class="language-julia hljs">estimator = Estimator(HC(2), QMC(), sample_lognormal, distributions)</code></pre><h2 id="Unbiased-estimation"><a class="docs-heading-anchor" href="#Unbiased-estimation">Unbiased estimation</a><a id="Unbiased-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Unbiased-estimation" title="Permalink"></a></h2><p>The most recent development to MultilevelEstimators is the addition of unbaised multilevel estimators. In contrast to standard Multilevel Monte Carlo, a weighted sum of multilevel contributions is used, thus eliminating the bias in the estimator.</p><p>This requires a small change to the sample function, in the sense that it should now return the solutions for all levels samller than or equal to <code>level</code>. In the PDE example this can easily be accomplished by using Full Multigrid, see [7] and [8]. See <a href="https://github.com/PieterjanRobbe/SimpleMultigrid.jl">SimpleMultigrid</a> and <a href="https://github.com/PieterjanRobbe/NotSoSimpleMultigrid.jl">NotSoSimpleMultigrid</a> for basic example solvers. See also <a href="https://github.com/PieterjanRobbe/LognormalDiffusionProblems.jl">LognormalDiffusionProblems</a> for a setup using Full Multigrid.</p><pre><code class="language-julia hljs">estimator = Estimator(U(1), MC(), sample_lognormal_fmg, distributions)</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Since this is a recent addition to MultilevelEstimators, it has not been tested as extensively as the other methods. Should you encounter any issues, feel free to submit an issue.</p></div></div><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>See the index set type <a href="manual.html#MultilevelEstimators.U"><code>U</code></a> for more information and set up.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="index.html">« Home</a><a class="docs-footer-nextpage" href="manual.html">Manual »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Wednesday 31 July 2024 22:53">Wednesday 31 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
